Clustering Algorithms
General Setup
python

import pandas as pd
import numpy as np
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans, AgglomerativeClustering
from sklearn.metrics import silhouette_score, cophenet
from scipy.cluster.hierarchy import dendrogram, linkage
from scipy.spatial.distance import pdist
import matplotlib.pyplot as plt
from yellowbrick.cluster import SilhouetteVisualizer

# Load Data
data = pd.read_csv('clust_data.csv')
if data.isnull().sum().any():
    data = data.fillna(data.mean())
data_normalized = (data - data.mean()) / data.std()
Perform PCA
python

pca = PCA()
pca_data = pca.fit_transform(data_normalized)

# Top 5 Eigenvalues and Eigenvectors
eigenvalues = pca.explained_variance_[:5]
eigenvectors = pca.components_[:5]
print("Top 5 Eigenvalues:", eigenvalues)
print("Top 5 Eigenvectors:\n", eigenvectors)

# Reduce dimensions to explain 90% variance
explained_variance_ratio = np.cumsum(pca.explained_variance_ratio_)
num_components = np.argmax(explained_variance_ratio >= 0.90) + 1
pca_90 = pca_data[:, :num_components]
K-Means with Silhouette Score
python

sil_scores = []
for n_clusters in range(2, 11):
    kmeans = KMeans(n_clusters=n_clusters, random_state=42)
    labels = kmeans.fit_predict(pca_90)
    sil_scores.append(silhouette_score(pca_90, labels))

optimal_clusters = np.argmax(sil_scores) + 2
print("Optimal Clusters:", optimal_clusters)

visualizer = SilhouetteVisualizer(KMeans(optimal_clusters, random_state=42), colors='yellowbrick')
visualizer.fit(pca_90)
visualizer.show()
Dendrogram and Cophenetic Correlation
python

linkage_methods = ['single', 'complete', 'average', 'ward']
cophenetic_scores = {}
for method in linkage_methods:
    Z = linkage(pca_90, method=method)
    coph_score, _ = cophenet(Z, pdist(pca_90))
    cophenetic_scores[method] = coph_score
    plt.figure()
    plt.title(f"Dendrogram ({method} linkage)")
    dendrogram(Z, truncate_mode='lastp', p=100)
    plt.show()

print("Cophenetic Correlation Coefficients:", cophenetic_scores)
Clustering Without PCA
python

kmeans_no_pca = KMeans(n_clusters=optimal_clusters, random_state=42)
agglo_no_pca = AgglomerativeClustering(n_clusters=optimal_clusters)

kmeans_labels = kmeans_no_pca.fit_predict(data_normalized)
agglo_labels = agglo_no_pca.fit_predict(data_normalized)

print("Silhouette Scores:")
print("K-Means:", silhouette_score(data_normalized, kmeans_labels))
print("Agglomerative Clustering:", silhouette_score(data_normalized, agglo_labels))
Market Basket Analysis with Apriori
Apriori Algorithm
python

from mlxtend.frequent_patterns import apriori, association_rules
from mlxtend.preprocessing import TransactionEncoder

# Define Transactions
transactions = [
    ["BREAD", "MILK", "BISCUIT", "CORNFLAKES"],
    ["BREAD", "TEA", "BOURNVITA"],
    ["JAM", "MAGGI", "BREAD", "MILK"],
    ["MAGGI", "TEA", "BISCUIT"],
    ["BREAD", "TEA", "BOURNVITA"]
]

# Preprocess Transactions
te = TransactionEncoder()
transformed_data = te.fit(transactions).transform(transactions)
df = pd.DataFrame(transformed_data, columns=te.columns_)

# Apply Apriori and Generate Rules
frequent_itemsets = apriori(df, min_support=0.1, use_colnames=True)
rules = association_rules(frequent_itemsets, metric="lift", min_threshold=2)

print("Frequent Itemsets:\n", frequent_itemsets)
print("\nRules with Lift > 2:\n", rules[rules['lift'] > 2])
Collaborative Filtering Recommendation Engine
SVD-Based Recommendation System
python

from surprise import SVD, Dataset, Reader
from surprise.model_selection import train_test_split

# Movie Metadata
movies = pd.DataFrame({
    "movieId": [1, 2, 3, 4, 5],
    "title": ["Toy Story (1995)", "Jumanji (1995)", "Grumpier Old Men (1995)", "Waiting to Exhale (1995)", "Father of the Bride Part II (1995)"]
})

# Ratings Data
ratings_data = {
    "userId": [1, 1, 1, 2, 2, 3, 3, 3],
    "movieId": [1, 2, 3, 1, 4, 2, 3, 5],
    "rating": [5.0, 4.0, 3.0, 4.0, 5.0, 2.0, 4.0, 5.0]
}
ratings = pd.DataFrame(ratings_data)

# Load Data into Surprise
reader = Reader(rating_scale=(0.5, 5.0))
data = Dataset.load_from_df(ratings[['userId', 'movieId', 'rating']], reader)

# Train/Test Split
trainset, testset = train_test_split(data, test_size=0.2, random_state=42)
svd = SVD()
svd.fit(trainset)

# Predict Top 5 Recommendations for User 1
user_id = 1
all_movie_ids = movies['movieId'].unique()
user_rated_movies = ratings[ratings['userId'] == user_id]['movieId']

unrated_movies = [movie for movie in all_movie_ids if movie not in user_rated_movies]
predicted_ratings = [(movie, svd.predict(user_id, movie).est) for movie in unrated_movies]
top_5_recommendations = sorted(predicted_ratings, key=lambda x: x[1], reverse=True)[:5]

print("Top 5 Recommendations for User 1:")
for movie, rating in top_5_recommendations:
    title = movies[movies['movieId'] == movie]['title'].values[0]
    print(f"{title} - Predicted Rating: {rating:.2f}")
